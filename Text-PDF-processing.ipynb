{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pytesseract \n",
    "from pdf2image import convert_from_path\n",
    "from transformers import pipeline\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Update this path based on your installation\n",
    "\n",
    "sensitive_words = [\"SSN\", \"credit card\", \"address\", \"phone number\", \"email\", \"passport\", \"bank account\"]\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for page in images:\n",
    "            text += pytesseract.image_to_string(page) + \"\\n\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sensitive_words(text):\n",
    "    pattern = r\"\\b(\" + \"|\".join(map(re.escape, sensitive_words)) + r\")\\b\"\n",
    "    return re.findall(pattern, text, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obfuscate_text(text, sensitive_words):\n",
    "    for word in sensitive_words:\n",
    "        text = re.sub(r'\\b' + re.escape(word) + r'\\b', 'REDACTED', text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_sensitive_data_with_ner(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    sensitive_entities = []\n",
    "    for entity in entities:\n",
    "        if entity['entity_group'] in ['PER', 'ORG', 'LOC', 'MISC']:\n",
    "            sensitive_entities.append(entity['word'])\n",
    "    return sensitive_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdf_from_text(output_pdf_path, obfuscated_text):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    fixed_width = 190 \n",
    "    for line in obfuscated_text.split('\\n'):\n",
    "        pdf.multi_cell(fixed_width, 10, line.encode('latin-1', 'replace').decode('latin-1'))\n",
    "    pdf.output(output_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_for_redaction(pdf_path, output_pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    text_chunks = chunk_text(text)\n",
    "    obfuscated_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        found_sensitive = find_sensitive_words(chunk)\n",
    "        if not found_sensitive:\n",
    "            found_sensitive = identify_sensitive_data_with_ner(chunk)\n",
    "        obfuscated_chunk = obfuscate_text(chunk, found_sensitive)\n",
    "        obfuscated_chunks.append(obfuscated_chunk)\n",
    "    obfuscated_text = \"\\n\".join(obfuscated_chunks)\n",
    "    create_pdf_from_text(output_pdf_path, obfuscated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_pdf_path = \"input.pdf\"\n",
    "    output_pdf_path = \"output_redacted.pdf\"\n",
    "    process_pdf_for_redaction(input_pdf_path, output_pdf_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
