{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing + OCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ramesh Kumar$ Aadhaar number is 1234-5678-9012, issued on January 15, 1990. He lives at\n",
      "123 Main Street, Bangalore, Karnataka: His mobile number is 98765-43210, and his email is\n",
      "ramesh kumar@example com_\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import easyocr\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    denoised_img = cv2.fastNlMeansDenoising(gray, None, 30, 7, 21)\n",
    "        \n",
    "    return denoised_img\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    image_path = r\"image.png\"  \n",
    "    \n",
    "\n",
    "    processed_image = preprocess_image(image_path)\n",
    "    \n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    result = reader.readtext(processed_image)\n",
    "    result_str = \"\"\n",
    "    \n",
    "    for detection in result:\n",
    "        print(detection[1])\n",
    "        result_str += detection[1] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token  |  Predicted Label\n",
      "-------------------------\n",
      "[CLS]  |  B-CARD\n",
      "'  |  O\n",
      "ram  |  O\n",
      "##esh  |  O\n",
      "kumar  |  O\n",
      "$  |  O\n",
      "aa  |  O\n",
      "##dha  |  O\n",
      "##ar  |  O\n",
      "number  |  O\n",
      "is  |  B-CARD\n",
      "123  |  O\n",
      "##4  |  O\n",
      "-  |  O\n",
      "56  |  O\n",
      "##7  |  O\n",
      "##8  |  O\n",
      "-  |  O\n",
      "90  |  O\n",
      "##12  |  O\n",
      ",  |  B-CARD\n",
      "issued  |  O\n",
      "on  |  O\n",
      "january  |  O\n",
      "15  |  O\n",
      ",  |  B-CARD\n",
      "1990  |  O\n",
      ".  |  B-CARD\n",
      "he  |  O\n",
      "lives  |  O\n",
      "at  |  O\n",
      "##12  |  O\n",
      "##3  |  O\n",
      "main  |  O\n",
      "street  |  O\n",
      ",  |  B-CARD\n",
      "bangalore  |  O\n",
      ",  |  B-CARD\n",
      "karnataka  |  O\n",
      ":  |  O\n",
      "his  |  O\n",
      "mobile  |  O\n",
      "number  |  O\n",
      "is  |  O\n",
      "98  |  O\n",
      "##7  |  O\n",
      "##65  |  O\n",
      "-  |  O\n",
      "43  |  O\n",
      "##21  |  O\n",
      "##0  |  O\n",
      ",  |  B-CARD\n",
      "and  |  O\n",
      "his  |  O\n",
      "email  |  O\n",
      "is  |  O\n",
      "##ram  |  O\n",
      "##esh  |  O\n",
      "kumar  |  O\n",
      "@  |  O\n",
      "example  |  O\n",
      "com  |  O\n",
      "_  |  O\n",
      "[SEP]  |  B-CARD\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('./saved_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./saved_model')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def predict(text, model, tokenizer, label_list):\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", is_split_into_words=False, padding=True, truncation=True)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "    \n",
    "    predicted_labels = [label_list[prediction.item()] for prediction in predictions[0]]\n",
    "\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    result = list(zip(tokens, predicted_labels))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_list = ['O', 'B-NAME', 'I-NAME', 'B-AADHAAR', 'I-AADHAAR', 'B-DL', 'I-DL', \n",
    "              'B-PASSPORT', 'I-PASSPORT', 'B-DATE', 'I-DATE', 'B-ADDRESS', 'I-ADDRESS', \n",
    "              'B-MOBILE', 'I-MOBILE', 'B-EMAIL', 'I-EMAIL', 'B-BANK', 'I-BANK', \n",
    "              'B-CC', 'I-CC', 'B-MEDICAL', 'I-MEDICAL', 'B-LOAN', 'I-LOAN', \n",
    "              'B-PIN', 'I-PIN', 'B-OTP', 'I-OTP', 'B-FINANCIAL', 'I-FINANCIAL', \n",
    "              'B-IP', 'I-IP', 'B-LOGIN', 'I-LOGIN', 'B-COOKIES', 'I-COOKIES', \n",
    "              'B-CREDIT', 'I-CREDIT', 'B-INSURANCE', 'I-INSURANCE', 'B-GENETIC', \n",
    "              'I-GENETIC', 'B-BIOMETRIC', 'I-BIOMETRIC', 'B-CARD', 'I-CARD']\n",
    "\n",
    "\n",
    "predictions = predict(result_str, model, tokenizer, label_list)\n",
    "\n",
    "\n",
    "print(\"Token  |  Predicted Label\")\n",
    "print(\"-------------------------\")\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}  |  {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=['Token', 'Predicted Label'])\n",
    "df.to_csv(\"ouput.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
